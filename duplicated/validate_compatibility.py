#!/usr/bin/env python3
"""
éªŒè¯EPIC3Dç”Ÿæˆæ•°æ®ä¸ç°æœ‰è®­ç»ƒæ¡†æ¶çš„å…¼å®¹æ€§

æµ‹è¯•ç”Ÿæˆçš„HDF5æ–‡ä»¶æ˜¯å¦èƒ½è¢«trainer_ddp.pyå’Œdata_collector.pyæ­£ç¡®åŠ è½½
"""

import os
import sys
import h5py
import numpy as np
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥ç°æœ‰çš„è®­ç»ƒæ¡†æ¶ç»„ä»¶
try:
    from offline.graph_buffer import GraphBuffer
except ImportError:
    print("Warning: æ— æ³•å¯¼å…¥graph_bufferï¼Œè·³è¿‡bufferå…¼å®¹æ€§æµ‹è¯•")
    GraphBuffer = None

def setup_logging():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    return logging.getLogger(__name__)

def load_merged_batch_files(file_paths):
    """
    æ¨¡æ‹Ÿtrainer_ddp.pyä¸­çš„load_merged_batch_fileså‡½æ•°
    """
    logger = logging.getLogger(__name__)
    
    all_data = {}
    total_samples = 0
    
    for file_path in file_paths:
        if not os.path.exists(file_path):
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            continue
            
        logger.info(f"åŠ è½½æ‰¹æ¬¡æ–‡ä»¶: {file_path}")
        
        try:
            with h5py.File(file_path, 'r') as f:
                # æ£€æŸ¥å¿…éœ€çš„æ•°æ®å­—æ®µ
                required_fields = [
                    'node_inputs', 'adj_list', 'node_padding_mask', 
                    'current_index', 'viewpoints', 'viewpoint_padding_mask',
                    'actions', 'rewards', 'dones'
                ]
                
                batch_data = {}
                for field in required_fields:
                    if field in f:
                        batch_data[field] = f[field][:]
                        logger.info(f"  - {field}: {batch_data[field].shape} ({batch_data[field].dtype})")
                    else:
                        logger.error(f"  - ç¼ºå°‘å­—æ®µ: {field}")
                        return None
                
                # ç´¯ç§¯æ•°æ®
                if not all_data:
                    all_data = batch_data
                else:
                    for field in required_fields:
                        all_data[field] = np.concatenate([all_data[field], batch_data[field]], axis=0)
                
                batch_samples = len(batch_data['actions'])
                total_samples += batch_samples
                logger.info(f"  - æ ·æœ¬æ•°: {batch_samples}")
                
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            return None
    
    logger.info(f"æ€»å…±åŠ è½½äº† {total_samples} ä¸ªæ ·æœ¬")
    return all_data

def validate_data_format(data):
    """éªŒè¯æ•°æ®æ ¼å¼æ˜¯å¦ç¬¦åˆé¢„æœŸ"""
    logger = logging.getLogger(__name__)
    logger.info("éªŒè¯æ•°æ®æ ¼å¼...")
    
    # æ£€æŸ¥åŸºæœ¬ç»´åº¦
    expected_dims = {
        'node_inputs': 3,      # (T, max_nodes, node_feature_dim)
        'adj_list': 3,         # (T, max_nodes, k_size)
        'node_padding_mask': 3, # (T, 1, max_nodes)
        'current_index': 3,    # (T, 1, 1)
        'viewpoints': 3,       # (T, max_viewpoints, 1)
        'viewpoint_padding_mask': 3, # (T, 1, max_viewpoints)
        'actions': 1,          # (T,)
        'rewards': 1,          # (T,)
        'dones': 1            # (T,)
    }
    
    T = len(data['actions'])
    logger.info(f"æ—¶é—´æ­¥æ•°: {T}")
    
    validation_passed = True
    
    for field, expected_ndim in expected_dims.items():
        if field not in data:
            logger.error(f"ç¼ºå°‘å­—æ®µ: {field}")
            validation_passed = False
            continue
            
        actual_shape = data[field].shape
        actual_ndim = len(actual_shape)
        
        if actual_ndim != expected_ndim:
            logger.error(f"{field}: ç»´åº¦ä¸åŒ¹é… - æœŸæœ›{expected_ndim}D, å®é™…{actual_ndim}D ({actual_shape})")
            validation_passed = False
        else:
            if actual_shape[0] != T:
                logger.error(f"{field}: ç¬¬ä¸€ç»´åº¦ä¸åŒ¹é… - æœŸæœ›{T}, å®é™…{actual_shape[0]}")
                validation_passed = False
            else:
                logger.info(f"âœ“ {field}: {actual_shape} ({data[field].dtype})")
    
    # æ£€æŸ¥ç‰¹å®šçš„ç»´åº¦çº¦æŸ
    if validation_passed:
        # current_index åº”è¯¥æ˜¯ (T, 1, 1)
        if data['current_index'].shape[1:] != (1, 1):
            logger.error(f"current_index ç»´åº¦é”™è¯¯: {data['current_index'].shape}")
            validation_passed = False
            
        # node_padding_mask åº”è¯¥æ˜¯ (T, 1, max_nodes)
        if len(data['node_padding_mask'].shape) == 3 and data['node_padding_mask'].shape[1] != 1:
            logger.error(f"node_padding_mask ç¬¬äºŒç»´åº¦åº”è¯¥æ˜¯1: {data['node_padding_mask'].shape}")
            validation_passed = False
            
        # viewpoint_padding_mask åº”è¯¥æ˜¯ (T, 1, max_viewpoints)
        if len(data['viewpoint_padding_mask'].shape) == 3 and data['viewpoint_padding_mask'].shape[1] != 1:
            logger.error(f"viewpoint_padding_mask ç¬¬äºŒç»´åº¦åº”è¯¥æ˜¯1: {data['viewpoint_padding_mask'].shape}")
            validation_passed = False
            
        # viewpoints åº”è¯¥æ˜¯ (T, max_viewpoints, 1)
        if len(data['viewpoints'].shape) == 3 and data['viewpoints'].shape[2] != 1:
            logger.error(f"viewpoints ç¬¬ä¸‰ç»´åº¦åº”è¯¥æ˜¯1: {data['viewpoints'].shape}")
            validation_passed = False
    
    if validation_passed:
        logger.info("âœ… æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡ï¼")
    else:
        logger.error("âŒ æ•°æ®æ ¼å¼éªŒè¯å¤±è´¥ï¼")
    
    return validation_passed

def test_graph_buffer_compatibility(data):
    """æµ‹è¯•ä¸GraphBufferçš„å…¼å®¹æ€§"""
    logger = logging.getLogger(__name__)
    
    if GraphBuffer is None:
        logger.warning("è·³è¿‡GraphBufferå…¼å®¹æ€§æµ‹è¯• (æ— æ³•å¯¼å…¥)")
        return True
    
    logger.info("æµ‹è¯•GraphBufferå…¼å®¹æ€§...")
    
    try:
        # å°è¯•åˆ›å»ºGraphBufferå®ä¾‹å¹¶æ·»åŠ æ•°æ®
        buffer = GraphBuffer()
        
        # æ¨¡æ‹Ÿæ·»åŠ æ•°æ®çš„è¿‡ç¨‹
        # æ³¨æ„ï¼šè¿™é‡Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…çš„GraphBuffer APIè¿›è¡Œè°ƒæ•´
        sample_data = {
            'node_inputs': data['node_inputs'][0],
            'adj_list': data['adj_list'][0],  
            'node_padding_mask': data['node_padding_mask'][0],
            'current_index': data['current_index'][0],
            'viewpoints': data['viewpoints'][0],
            'viewpoint_padding_mask': data['viewpoint_padding_mask'][0],
            'action': data['actions'][0],
            'reward': data['rewards'][0], 
            'done': data['dones'][0]
        }
        
        # è¿™é‡Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…çš„buffer.addæ–¹æ³•ç­¾åè¿›è¡Œè°ƒæ•´
        # buffer.add(sample_data)
        
        logger.info("âœ… GraphBufferå…¼å®¹æ€§æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ GraphBufferå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    logger = setup_logging()
    logger.info("å¼€å§‹EPIC3Dæ•°æ®å…¼å®¹æ€§éªŒè¯...")
    
    # æµ‹è¯•æ–‡ä»¶è·¯å¾„
    test_outputs_dir = "/home/amax/EPIC/src/offline/test_outputs"
    
    # 1. æµ‹è¯•å•episodeæ‰¹æ¬¡æ–‡ä»¶åŠ è½½
    logger.info("\n=== æµ‹è¯•å•episodeæ‰¹æ¬¡æ–‡ä»¶åŠ è½½ ===")
    single_batch_file = os.path.join(test_outputs_dir, "dungeon_test_episode_batch_1.h5")
    
    if not os.path.exists(single_batch_file):
        logger.error(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {single_batch_file}")
        return False
    
    single_data = load_merged_batch_files([single_batch_file])
    if single_data is None:
        logger.error("å•episodeæ‰¹æ¬¡æ–‡ä»¶åŠ è½½å¤±è´¥")
        return False
    
    single_format_valid = validate_data_format(single_data)
    
    # 2. æµ‹è¯•å¤šepisodeæ‰¹æ¬¡æ–‡ä»¶åŠ è½½
    logger.info("\n=== æµ‹è¯•å¤šepisodeæ‰¹æ¬¡æ–‡ä»¶åŠ è½½ ===")
    multi_batch_files = [
        os.path.join(test_outputs_dir, "dungeon_multi_episodes_batch_1.h5"),
        os.path.join(test_outputs_dir, "dungeon_multi_episodes_batch_2.h5"),
        os.path.join(test_outputs_dir, "dungeon_multi_episodes_batch_3.h5")
    ]
    
    multi_data = load_merged_batch_files(multi_batch_files)
    if multi_data is None:
        logger.error("å¤šepisodeæ‰¹æ¬¡æ–‡ä»¶åŠ è½½å¤±è´¥")
        return False
    
    multi_format_valid = validate_data_format(multi_data)
    
    # 3. æµ‹è¯•GraphBufferå…¼å®¹æ€§
    logger.info("\n=== æµ‹è¯•GraphBufferå…¼å®¹æ€§ ===")
    buffer_compatible = test_graph_buffer_compatibility(single_data)
    
    # 4. æ•°æ®ç»Ÿè®¡ä¿¡æ¯
    logger.info("\n=== æ•°æ®ç»Ÿè®¡ä¿¡æ¯ ===")
    logger.info(f"å•episodeæ•°æ®: {len(single_data['actions'])} æ ·æœ¬")
    logger.info(f"å¤šepisodeæ•°æ®: {len(multi_data['actions'])} æ ·æœ¬")
    logger.info(f"åŠ¨ä½œå”¯ä¸€å€¼æ•°é‡: {len(np.unique(multi_data['actions']))}")
    logger.info(f"å¥–åŠ±èŒƒå›´: [{np.min(multi_data['rewards']):.3f}, {np.max(multi_data['rewards']):.3f}]")
    
    # æ€»ç»“ç»“æœ
    logger.info("\n=== å…¼å®¹æ€§éªŒè¯æ€»ç»“ ===")
    
    all_tests_passed = single_format_valid and multi_format_valid and buffer_compatible
    
    if all_tests_passed:
        logger.info("ğŸ‰ æ‰€æœ‰å…¼å®¹æ€§æµ‹è¯•é€šè¿‡ï¼")
        logger.info("âœ… EPIC3Dç”Ÿæˆçš„æ•°æ®å®Œå…¨å…¼å®¹ç°æœ‰è®­ç»ƒæ¡†æ¶")
        logger.info("âœ… å¯ä»¥ç›´æ¥ç”¨äºtrainer_ddp.pyè¿›è¡Œè®­ç»ƒ")
        return True
    else:
        logger.error("âŒ éƒ¨åˆ†å…¼å®¹æ€§æµ‹è¯•å¤±è´¥")
        if not single_format_valid:
            logger.error("  - å•episodeæ ¼å¼éªŒè¯å¤±è´¥")
        if not multi_format_valid:
            logger.error("  - å¤šepisodeæ ¼å¼éªŒè¯å¤±è´¥")
        if not buffer_compatible:
            logger.error("  - GraphBufferå…¼å®¹æ€§æµ‹è¯•å¤±è´¥")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
