#!/usr/bin/env python3
"""
æ·±åº¦åˆ†ææ•°æ®é›†ä¸­çš„å¼‚å¸¸æ ·æœ¬
æ£€æŸ¥æ˜¯å¦å­˜åœ¨éepisodeç»ˆæ­¢æ€çš„æ— æ•ˆæ•°æ®
"""

import os
import h5py
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict, Counter
import argparse

def deep_analyze_batch_file(batch_file_path):
    """æ·±åº¦åˆ†æå•ä¸ªæ‰¹æ¬¡æ–‡ä»¶ä¸­çš„å¼‚å¸¸æ ·æœ¬"""
    anomalies = []
    
    try:
        with h5py.File(batch_file_path, 'r') as f:
            # è·å–æ‰€æœ‰æ•°æ®
            viewpoints = f['viewpoints'][:]  # [N, 25, 1]
            viewpoint_padding_mask = f['viewpoint_padding_mask'][:]  # [N, 1, 25]
            node_inputs = f['node_inputs'][:]  # [N, 2500, 9]
            node_padding_mask = f['node_padding_mask'][:]  # [N, 1, 2500]
            rewards = f['rewards'][:]
            actions = f['actions'][:]
            dones = f['dones'][:]
            current_index = f['current_index'][:]
            
            # é¢å¤–æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'episode_id' in f:
                episode_ids = f['episode_id'][:]
            else:
                episode_ids = np.zeros(len(viewpoints))
            
            if 'timestep' in f:
                timesteps = f['timestep'][:]
            else:
                timesteps = np.arange(len(viewpoints))
            
            N = len(viewpoints)
            
            for i in range(N):
                # è®¡ç®—æœ‰æ•ˆè§†ç‚¹æ•°å’Œæœ‰æ•ˆèŠ‚ç‚¹æ•°
                vp_mask = viewpoint_padding_mask[i, 0, :]  # [25]
                node_mask = node_padding_mask[i, 0, :]     # [2500]
                
                valid_vps = np.sum(~vp_mask)  # Falseè¡¨ç¤ºæœ‰æ•ˆ
                valid_nodes = np.sum(~node_mask)
                
                # åˆ†æå¼‚å¸¸æƒ…å†µ
                anomaly_info = {
                    'sample_idx': i,
                    'batch_file': os.path.basename(batch_file_path),
                    'episode_id': episode_ids[i] if len(episode_ids) > i else -1,
                    'timestep': timesteps[i] if len(timesteps) > i else -1,
                    'action': actions[i],
                    'reward': rewards[i],
                    'done': dones[i],
                    'current_index': current_index[i, 0, 0] if current_index.shape[-1] > 0 else -1,
                    'valid_vps': valid_vps,
                    'valid_nodes': valid_nodes,
                    'anomalies': []
                }
                
                # æ£€æŸ¥å„ç§å¼‚å¸¸æƒ…å†µ
                
                # 1. valid_vps=0 ä½†ä¸æ˜¯doneçŠ¶æ€
                if valid_vps == 0 and not dones[i]:
                    anomaly_info['anomalies'].append("valid_vps=0_but_not_done")
                
                # 2. valid_vps=0 ä½†åœ¨episodeä¸­é—´ä½ç½®
                if valid_vps == 0 and i < N - 1:  # ä¸æ˜¯æœ€åä¸€ä¸ªæ ·æœ¬
                    # æ£€æŸ¥åç»­æ ·æœ¬æ˜¯å¦è¿˜æœ‰æ•°æ®
                    if i < N - 5:  # è¿˜æœ‰5ä¸ªä»¥ä¸Šåç»­æ ·æœ¬
                        anomaly_info['anomalies'].append("valid_vps=0_in_middle")
                
                # 3. æœ‰æ•ˆèŠ‚ç‚¹æ•°ä¸º0
                if valid_nodes == 0:
                    anomaly_info['anomalies'].append("valid_nodes=0")
                
                # 4. current_indexè¶…å‡ºæœ‰æ•ˆèŠ‚ç‚¹èŒƒå›´
                if anomaly_info['current_index'] >= valid_nodes and valid_nodes > 0:
                    anomaly_info['anomalies'].append("current_index_out_of_range")
                
                # 5. actionè¶…å‡ºæœ‰æ•ˆè§†ç‚¹èŒƒå›´
                if actions[i] >= valid_vps and valid_vps > 0:
                    anomaly_info['anomalies'].append("action_out_of_range")
                
                # 6. èŠ‚ç‚¹ç‰¹å¾å¼‚å¸¸
                node_features = node_inputs[i]
                valid_node_features = node_features[~node_mask]
                if len(valid_node_features) > 0:
                    # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Inf
                    if np.isnan(valid_node_features).any():
                        anomaly_info['anomalies'].append("node_features_nan")
                    if np.isinf(valid_node_features).any():
                        anomaly_info['anomalies'].append("node_features_inf")
                    
                    # æ£€æŸ¥ç‰¹å¾å€¼èŒƒå›´æ˜¯å¦åˆç†
                    feature_max = np.max(np.abs(valid_node_features))
                    if feature_max > 1000:  # ç‰¹å¾å€¼è¿‡å¤§
                        anomaly_info['anomalies'].append(f"node_features_too_large_{feature_max:.1f}")
                
                # 7. è§†ç‚¹æ•°æ®å¼‚å¸¸
                valid_viewpoint_data = viewpoints[i][~vp_mask]
                if len(valid_viewpoint_data) > 0:
                    vp_max = np.max(valid_viewpoint_data)
                    vp_min = np.min(valid_viewpoint_data)
                    
                    # è§†ç‚¹ç´¢å¼•åº”è¯¥åœ¨æœ‰æ•ˆèŠ‚ç‚¹èŒƒå›´å†…
                    if vp_max >= valid_nodes:
                        anomaly_info['anomalies'].append("viewpoint_index_out_of_node_range")
                    if vp_min < 0:
                        anomaly_info['anomalies'].append("viewpoint_index_negative")
                
                # 8. å¥–åŠ±å¼‚å¸¸
                if np.isnan(rewards[i]) or np.isinf(rewards[i]):
                    anomaly_info['anomalies'].append("reward_nan_inf")
                if abs(rewards[i]) > 10:  # å¥–åŠ±è¿‡å¤§
                    anomaly_info['anomalies'].append(f"reward_too_large_{rewards[i]:.3f}")
                
                # åªè®°å½•æœ‰å¼‚å¸¸çš„æ ·æœ¬
                if anomaly_info['anomalies']:
                    anomalies.append(anomaly_info)
    
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶ {batch_file_path} æ—¶å‡ºé”™: {e}")
        return []
    
    return anomalies

def analyze_dataset_anomalies(data_path):
    """åˆ†ææ•´ä¸ªæ•°æ®é›†çš„å¼‚å¸¸æƒ…å†µ"""
    print(f"ğŸ” æ·±åº¦åˆ†ææ•°æ®é›†å¼‚å¸¸: {data_path}")
    
    # è·å–æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶
    batch_files = []
    for f in os.listdir(data_path):
        if f.endswith('.h5') and ('batch' in f or f.startswith('dataset_batch_')):
            batch_files.append(os.path.join(data_path, f))
    
    batch_files.sort()
    print(f"ğŸ“ æ‰¾åˆ° {len(batch_files)} ä¸ªæ‰¹æ¬¡æ–‡ä»¶")
    
    all_anomalies = []
    anomaly_stats = Counter()
    
    # åˆ†ææ¯ä¸ªæ–‡ä»¶
    for i, batch_file in enumerate(batch_files):
        if i % 20 == 0:  # åªæ˜¾ç¤ºè¿›åº¦
            print(f"ğŸ“Š å¤„ç†æ–‡ä»¶ {i+1}/{len(batch_files)}")
        
        anomalies = deep_analyze_batch_file(batch_file)
        all_anomalies.extend(anomalies)
        
        # ç»Ÿè®¡å¼‚å¸¸ç±»å‹
        for anomaly in anomalies:
            for anomaly_type in anomaly['anomalies']:
                anomaly_stats[anomaly_type] += 1
    
    return all_anomalies, anomaly_stats

def analyze_episode_structure(anomalies):
    """åˆ†æå¼‚å¸¸æ ·æœ¬çš„episodeç»“æ„"""
    print(f"\nğŸ”¬ åˆ†æå¼‚å¸¸æ ·æœ¬çš„episodeç»“æ„:")
    
    # æŒ‰batchæ–‡ä»¶åˆ†ç»„
    file_anomalies = defaultdict(list)
    for anomaly in anomalies:
        file_anomalies[anomaly['batch_file']].append(anomaly)
    
    problematic_files = []
    
    for batch_file, file_anomalies_list in file_anomalies.items():
        # æŒ‰sample_idxæ’åº
        file_anomalies_list.sort(key=lambda x: x['sample_idx'])
        
        print(f"\nğŸ“„ æ–‡ä»¶: {batch_file}")
        print(f"   å¼‚å¸¸æ ·æœ¬æ•°: {len(file_anomalies_list)}")
        
        # æ£€æŸ¥å¼‚å¸¸æ ·æœ¬çš„åˆ†å¸ƒ
        sample_indices = [a['sample_idx'] for a in file_anomalies_list]
        if sample_indices:
            print(f"   å¼‚å¸¸æ ·æœ¬ä½ç½®: {min(sample_indices)} - {max(sample_indices)}")
            
            # æ£€æŸ¥æ˜¯å¦åœ¨episodeæœ«å°¾
            max_idx = max(sample_indices)
            
            # å°è¯•è·å–æ–‡ä»¶æ€»æ ·æœ¬æ•°
            try:
                with h5py.File(os.path.join('/home/amax/EPIC/datasets', batch_file), 'r') as f:
                    total_samples = len(f['actions'])
                
                end_ratio = max_idx / total_samples
                print(f"   æœ€å¤§å¼‚å¸¸ä½ç½®æ¯”ä¾‹: {end_ratio:.1%} ({max_idx}/{total_samples})")
                
                if end_ratio < 0.9:  # å¼‚å¸¸å‡ºç°åœ¨å‰90%
                    problematic_files.append({
                        'file': batch_file,
                        'anomalies': file_anomalies_list,
                        'end_ratio': end_ratio,
                        'total_samples': total_samples
                    })
                    print(f"   âš ï¸  å¼‚å¸¸å‡ºç°åœ¨episodeä¸­é—´ï¼Œéœ€è¦æ·±å…¥æ£€æŸ¥")
            except:
                print(f"   âŒ æ— æ³•è¯»å–æ–‡ä»¶æ€»æ ·æœ¬æ•°")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªå¼‚å¸¸æ ·æœ¬çš„è¯¦æƒ…
        for j, anomaly in enumerate(file_anomalies_list[:3]):
            anomaly_types_str = ", ".join(anomaly['anomalies'])
            print(f"     æ ·æœ¬ {anomaly['sample_idx']}: {anomaly_types_str}")
            print(f"       valid_vps={anomaly['valid_vps']}, done={anomaly['done']}, "
                  f"action={anomaly['action']}, reward={anomaly['reward']:.4f}")
    
    return problematic_files

def detailed_file_analysis(file_path):
    """å¯¹é—®é¢˜æ–‡ä»¶è¿›è¡Œè¯¦ç»†åˆ†æ"""
    print(f"\nğŸ” è¯¦ç»†åˆ†æé—®é¢˜æ–‡ä»¶: {os.path.basename(file_path)}")
    
    try:
        with h5py.File(file_path, 'r') as f:
            viewpoint_padding_mask = f['viewpoint_padding_mask'][:]
            dones = f['dones'][:]
            rewards = f['rewards'][:]
            actions = f['actions'][:]
            
            N = len(dones)
            
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆè§†ç‚¹æ•°
            valid_vps_sequence = []
            for i in range(N):
                vp_mask = viewpoint_padding_mask[i, 0, :]
                valid_vps = np.sum(~vp_mask)
                valid_vps_sequence.append(valid_vps)
            
            # æ‰¾åˆ°æ‰€æœ‰valid_vps=0çš„ä½ç½®
            zero_vps_positions = [i for i, vps in enumerate(valid_vps_sequence) if vps == 0]
            done_positions = [i for i, done in enumerate(dones) if done]
            
            print(f"   æ€»æ ·æœ¬æ•°: {N}")
            print(f"   valid_vps=0çš„ä½ç½®: {zero_vps_positions}")
            print(f"   done=Trueçš„ä½ç½®: {done_positions}")
            
            # æ£€æŸ¥è¿ç»­çš„valid_vpså˜åŒ–
            print(f"\n   æœ‰æ•ˆè§†ç‚¹æ•°å˜åŒ–åºåˆ—ï¼ˆæ˜¾ç¤ºå‰20ä¸ªå’Œå20ä¸ªï¼‰:")
            for i in range(min(20, N)):
                status = ""
                if i in zero_vps_positions:
                    status += " [ZERO_VPS]"
                if i in done_positions:
                    status += " [DONE]"
                print(f"     æ ·æœ¬ {i:3d}: valid_vps={valid_vps_sequence[i]:2d}, done={dones[i]}, "
                      f"action={actions[i]:2d}, reward={rewards[i]:6.3f}{status}")
            
            if N > 40:
                print("     ...")
                for i in range(max(20, N-20), N):
                    status = ""
                    if i in zero_vps_positions:
                        status += " [ZERO_VPS]"
                    if i in done_positions:
                        status += " [DONE]"
                    print(f"     æ ·æœ¬ {i:3d}: valid_vps={valid_vps_sequence[i]:2d}, done={dones[i]}, "
                          f"action={actions[i]:2d}, reward={rewards[i]:6.3f}{status}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰éç»ˆæ­¢ä½ç½®çš„zero_vps
            non_terminal_zero_vps = []
            for pos in zero_vps_positions:
                if pos < N - 1 and not dones[pos]:  # ä¸æ˜¯æœ€åä¸€ä¸ªä¸”ä¸æ˜¯doneçŠ¶æ€
                    non_terminal_zero_vps.append(pos)
            
            if non_terminal_zero_vps:
                print(f"\n   âš ï¸  å‘ç° {len(non_terminal_zero_vps)} ä¸ªéç»ˆæ­¢ä½ç½®çš„zero_vps:")
                for pos in non_terminal_zero_vps:
                    print(f"     ä½ç½® {pos}: valid_vps=0, done={dones[pos]}, åç»­è¿˜æœ‰{N-pos-1}ä¸ªæ ·æœ¬")
    
    except Exception as e:
        print(f"âŒ è¯¦ç»†åˆ†æå¤±è´¥: {e}")

def main():
    parser = argparse.ArgumentParser(description='æ·±åº¦åˆ†ææ•°æ®é›†ä¸­çš„å¼‚å¸¸æ ·æœ¬')
    parser.add_argument('--data_path', default='/home/amax/EPIC/datasets_v3', 
                       help='æ•°æ®é›†ç›®å½•è·¯å¾„')
    parser.add_argument('--detailed_analysis', action='store_true',
                       help='å¯¹é—®é¢˜æ–‡ä»¶è¿›è¡Œè¯¦ç»†åˆ†æ')
    
    args = parser.parse_args()
    
    print("ğŸ” EPIC 3Dæ•°æ®é›†å¼‚å¸¸æ·±åº¦åˆ†æ")
    print("=" * 60)
    
    # åˆ†æå¼‚å¸¸
    all_anomalies, anomaly_stats = analyze_dataset_anomalies(args.data_path)
    
    print(f"\nğŸ“Š å¼‚å¸¸ç»Ÿè®¡:")
    print(f"  æ€»å¼‚å¸¸æ ·æœ¬æ•°: {len(all_anomalies)}")
    
    if not all_anomalies:
        print("âœ… æœªå‘ç°ä»»ä½•å¼‚å¸¸ï¼æ•°æ®é›†è´¨é‡è‰¯å¥½ã€‚")
        return
    
    print(f"\nğŸ“‹ å¼‚å¸¸ç±»å‹ç»Ÿè®¡:")
    for anomaly_type, count in anomaly_stats.most_common():
        print(f"  {anomaly_type}: {count} æ¬¡")
    
    # åˆ†æepisodeç»“æ„
    problematic_files = analyze_episode_structure(all_anomalies)
    
    if problematic_files:
        print(f"\nâš ï¸  å‘ç° {len(problematic_files)} ä¸ªå¯èƒ½æœ‰é—®é¢˜çš„æ–‡ä»¶:")
        for pf in problematic_files:
            print(f"  {pf['file']}: å¼‚å¸¸ä½ç½®æ¯”ä¾‹ {pf['end_ratio']:.1%}")
        
        # è¯¦ç»†åˆ†æç¬¬ä¸€ä¸ªé—®é¢˜æ–‡ä»¶
        if args.detailed_analysis and problematic_files:
            for i in range(len(problematic_files)):
                first_problem_file = os.path.join(args.data_path, problematic_files[i]['file'])
                detailed_file_analysis(first_problem_file)
    else:
        print("âœ… æ‰€æœ‰å¼‚å¸¸éƒ½å‡ºç°åœ¨episodeæœ«å°¾ï¼Œæ•°æ®ç»“æ„æ­£å¸¸ã€‚")
    
    print("\nâœ… åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main()
