import copy, torch, math, numpy as np
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from torch.nn.parallel import DistributedDataParallel as DDP

from model.graph_denoiser import create_pointer_denoiser
from model.diffusion_discrete import DiscreteDiffusion
from model.sgformer.sgformer import QNet                           # graph critic
from dataclasses import dataclass
from typing import Dict, Optional
import uuid
import os


@dataclass
class TrainConfig:
    """Diffusion-QLè®­ç»ƒé…ç½®ç±»"""
    
    # å®éªŒè®¾ç½®
    device: str = "cuda"
    seed: int = 0  
    save_freq: int = int(1e2)  # è¯„ä¼°é¢‘ç‡ï¼ˆæ­¥æ•°ï¼‰
    eval_start: int = 0  # å¼€å§‹è¯„ä¼°çš„æ­¥æ•°ï¼Œé»˜è®¤ä»ç¬¬0æ­¥å¼€å§‹
    
    # æ–°å¢ï¼šé‡‡æ ·é…ç½®
    temperature_start: float = 1.0  # åˆå§‹æ¸©åº¦
    temperature_end: float = 0.1  # æœ€ç»ˆæ¸©åº¦
    temperature_decay_steps: int = 50000  # æ¸©åº¦è¡°å‡æ­¥æ•°
    max_timesteps: int = int(3e5)  # è¿è¡Œç¯å¢ƒçš„æœ€å¤§æ—¶é—´æ­¥æ•°
    checkpoints_path: Optional[str] = None  # ä¿å­˜è·¯å¾„
    load_model_path: Optional[str] = None  # æ¨¡å‹åŠ è½½è·¯å¾„
    
    # ä¼˜åŒ–è®¾ç½®
    learning_rate: float = 3e-4
    batch_size: int = 256
    mini_batch_size: int = 32  
    discount: float = 0.99
    clip_grad_norm: float = 1.0
    
    # DDQLç‰¹æœ‰å‚æ•°
    actor_bc_coef: float = 1.0  # è¡Œä¸ºå…‹éš†æŸå¤±æƒé‡ï¼ˆåŸetaå‚æ•°ï¼‰
    n_timesteps: int = 20  # æ‰©æ•£æ­¥æ•°
    ema_decay: float = 0.995  # EMAè¡°å‡ç‡
    step_start_ema: int = 1000  # å¼€å§‹EMAçš„æ­¥æ•°
    update_ema_every: int = 5  # EMAæ›´æ–°é¢‘ç‡
    tau: float = 0.005  # ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°ç³»æ•°
    actor_update_freq: int = 1  # Actoræ›´æ–°é¢‘ç‡
        
    # æ–°å¢ï¼šç†µæ­£åˆ™åŒ–å‚æ•°
    entropy_reg_weight: float = 0.01  # ç†µæ­£åˆ™åŒ–æƒé‡Î»_H

    # æ–°å¢ï¼šcriticç¨³å®šåŒ–é…ç½®
    use_dataset_actions: bool = True  # æ˜¯å¦ä½¿ç”¨æ•°æ®é›†åŠ¨ä½œè€Œéactoré‡‡æ ·
    use_smooth_loss: bool = True  # æ˜¯å¦ä½¿ç”¨å¹³æ»‘æŸå¤±ï¼ˆHuberï¼‰
    huber_delta: float = 1.0  # HuberæŸå¤±çš„deltaå‚æ•°
    improved_off_value: float = -1e6  # æ”¹è¿›çš„maskå¡«å……å€¼
    
    # Wandbæ—¥å¿—
    project: str = "DDQL-Graph"
    group: str = "DDQL"
    name: str = "DDQL"
    
    # å›¾ç›¸å…³å‚æ•°
    node_dim: int = 6  # èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
    num_nodes: int = None  # å›¾ä¸­èŠ‚ç‚¹æ•°é‡
    edge_dim: int = None  # è¾¹ç‰¹å¾ç»´åº¦
    gnn_hidden_dim: int = 128  # å›¾ç¥ç»ç½‘ç»œéšè—å±‚ç»´åº¦
    viewpoint_padding_size: int = 180  # è§†ç‚¹å¡«å……å¤§å°/åŠ¨ä½œç©ºé—´ç»´åº¦
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        self.name = f"{self.name}-{str(uuid.uuid4())[:8]}"
        if self.checkpoints_path is not None:
            self.checkpoints_path = os.path.join(self.checkpoints_path, self.name)
    
    def update(self, **kwargs):
        """æ›´æ–°é…ç½®å‚æ•°"""
        for k, v in kwargs.items():
            if hasattr(self, k):
                setattr(self, k, v)
            else:
                print(f"è­¦å‘Š: é…ç½®ä¸­ä¸å­˜åœ¨å‚æ•° '{k}'")
        return self

    def display(self):
        """æ˜¾ç¤ºå½“å‰é…ç½®"""
        print("====== DDQL-Graphè®­ç»ƒé…ç½® ======")
        for k, v in sorted(vars(self).items()):
            print(f"{k}: {v}")
        print("================================")


# ---------------------------- åŒ Q ç½‘ç»œå°è£… ---------------------------- #
class TwinQ(nn.Module):
    """Q1ã€Q2 ä¸¤ä¸ªç‹¬ç«‹çš„ Graph QNetï¼Œè¾“å‡º [batch_size, VIEWPOINT_PADDING_SIZE]"""
    def __init__(self, node_dim, embed_dim):
        super().__init__()
        self.q1 = QNet(node_dim, embed_dim)
        self.q2 = QNet(node_dim, embed_dim)

    def forward(self, obs):
        return self.q1(obs).squeeze(-1), self.q2(obs).squeeze(-1)  # each => [batch_size, VIEWPOINT_PADDING_SIZE]

    def q_min(self, obs, act_idx):
        q1, q2 = self.forward(obs)
        q1 = q1.gather(1, act_idx.unsqueeze(1))
        q2 = q2.gather(1, act_idx.unsqueeze(1))
        return torch.min(q1, q2)                    # [batch_size, 1]


# ---------------------------- EMA è¾…åŠ© ---------------------------- #
class EMA:
    def __init__(self, beta: float):
        self.beta = beta

    @torch.no_grad()
    def update(self, ema_m, cur_m):
        for p_ema, p_cur in zip(ema_m.parameters(), cur_m.parameters()):
            p_ema.data.mul_(self.beta).add_(p_cur.data, alpha=1 - self.beta)


# ======================= ä¸»ç®—æ³•ï¼šDiffusionGraphQL ===================== #
class DiffusionGraphQL:
    def __init__(
        self,
        actor: nn.Module,
        actor_opt: torch.optim.Optimizer,
        critic: nn.Module,
        critic_opt: torch.optim.Optimizer,
        device: str,
        discount: float     = 0.99,
        tau: float          = 0.005,
        actor_bc_coef: float = 1.0,
        ema_decay: float    = 0.995,
        step_start_ema: int = 1000,
        update_ema_every: int = 5,
        grad_clip: float    = 1.0,
        rank: int = 0,
        world_size: int = 1,
        actor_update_freq: int = 1,
        # Criticç¨³å®šåŒ–é…ç½®
        use_dataset_actions: bool = True,
        use_smooth_loss: bool = True,
        huber_delta: float = 1.0,
        # ç†µæ­£åˆ™åŒ–é…ç½®
        entropy_reg_weight: float = 0.01,
    ):
        self.device = device
        self.rank = rank
        self.world_size = world_size

        # ---------- Actor : Discrete Diffusion ---------- #
        self.actor = actor
        self.actor_opt = actor_opt

        # ---------- Critic : Twin Graph QNet ---------- #
        self.critic = critic
        self.critic_tgt = copy.deepcopy(self.critic)
        self.critic_opt = critic_opt

        # ---------- Actor & Critic Module ---------- #
        # æ·»åŠ è¿™äº›è¡Œä»¥ä¿å­˜åŸå§‹æ¨¡å‹å¼•ç”¨
        self.actor_module = actor.module if isinstance(actor, nn.parallel.DistributedDataParallel) else actor
        self.critic_module = critic.module if isinstance(critic, nn.parallel.DistributedDataParallel) else critic
        self.critic_tgt_module = self.critic_tgt.module if isinstance(self.critic_tgt, nn.parallel.DistributedDataParallel) else self.critic_tgt

        # ---------- EMA ---------- #
        self.ema = EMA(ema_decay)
        self.step_start_ema = step_start_ema
        self.update_ema_every = update_ema_every
        self.actor_update_freq = actor_update_freq

        # ---------- Hyper-params ---------- #
        self.discount = discount
        self.tau = tau
        self.actor_bc_coef = actor_bc_coef
        self.grad_clip = grad_clip
        
        # ä¿å­˜åˆå§‹å­¦ä¹ ç‡ï¼Œç”¨äºå­¦ä¹ ç‡è¡°å‡
        self.initial_actor_lr = self._get_lr(self.actor_opt)
        self.initial_critic_lr = self._get_lr(self.critic_opt)
        
        # ä¿å­˜åˆå§‹actor_bc_coefç”¨äºè¡°å‡
        self.initial_actor_bc_coef = actor_bc_coef

        # æ–°å¢ï¼šé‡‡æ ·é…ç½®
        self.temperature_start = getattr(actor, 'temperature_start', 1.0)
        self.temperature_end = getattr(actor, 'temperature_end', 0.1)
        self.temperature_decay_steps = getattr(actor, 'temperature_decay_steps', 50000)
        self.current_temperature = self.temperature_start

        # Criticç¨³å®šåŒ–é…ç½®
        self.use_dataset_actions = use_dataset_actions
        self.use_smooth_loss = use_smooth_loss
        self.huber_delta = huber_delta

        # ç†µæ­£åˆ™åŒ–
        self.entropy_reg_weight = entropy_reg_weight
        self.step = 0

    def compute_critic_loss(self, q1a: torch.Tensor, q2a: torch.Tensor, target_q: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—criticæŸå¤±ï¼Œæ”¯æŒå¹³æ»‘æŸå¤±"""
        if self.use_smooth_loss:
            # ä½¿ç”¨HuberæŸå¤±æ›¿ä»£MSEï¼Œå‡å°‘å¼‚å¸¸å€¼å½±å“
            loss1 = F.smooth_l1_loss(q1a, target_q, reduction="mean", beta=self.huber_delta)
            loss2 = F.smooth_l1_loss(q2a, target_q, reduction="mean", beta=self.huber_delta)
        else:
            # ä¼ ç»ŸMSEæŸå¤±
            loss1 = F.mse_loss(q1a, target_q)
            loss2 = F.mse_loss(q2a, target_q)
        
        return loss1 + loss2

    def compute_ql_loss_stable(self, action_probs, q_values, baseline_q):
        """
        ç¨³å®šçš„Advantage-weighted Q-learning lossï¼Œå¸¦ç†µæ­£åˆ™åŒ–ã€‚
        
        å‚æ•°:
            action_probs: åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ [B, K]
            q_values: Qå€¼ [B, K] 
            baseline_q: åŸºå‡†Qå€¼ [B, 1]
            
        è¿”å›:
            ç¨³å®šçš„QLæŸå¤±æ ‡é‡
        """
        # è®¡ç®—æœŸæœ›Qå€¼
        expected_q = (action_probs * q_values).sum(dim=-1, keepdim=True)  # [B, 1]
        
        # å…³é”®ä¿®å¤ï¼šå…ˆè®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ä¼˜åŠ¿ä¿¡å·ï¼Œå†å¯¹batchå–å‡å€¼
        adv = expected_q - baseline_q  # [B, 1] - ä¿ç•™æ¯ä¸ªæ ·æœ¬çš„å·®å¼‚ä¿¡æ¯
        
        # æ·»åŠ ç†µæ­£åˆ™åŒ–ï¼šé˜²æ­¢Actorç†µè¢«RL"å¸å¹²"
        # è®¡ç®—ç†µï¼šH = -Î£ p_i * log(p_i)
        log_probs = torch.log(action_probs + 1e-8)  # æ·»åŠ å°å€¼é˜²æ­¢log(0)
        entropy = -(action_probs * log_probs).sum(dim=-1, keepdim=True)  # [B, 1]
        
        # è¿”å›è´Ÿçš„å½’ä¸€åŒ–ä¼˜åŠ¿å‡å€¼åŠ ä¸Šç†µå¥–åŠ±ä½œä¸ºæŸå¤±ï¼ˆæ¢¯åº¦ä¸Šå‡è½¬ä¸ºæ¢¯åº¦ä¸‹é™ï¼‰
        ql_loss = (-adv - self.entropy_reg_weight * entropy).mean()
        
        return ql_loss, entropy.mean()  # è¿”å›æŸå¤±å’Œå¹³å‡ç†µç”¨äºæ—¥å¿—

    # ---------------------- è®­ç»ƒä¸€æ­¥ ---------------------- #
    def train(self, batch, mini_batch: int = None, progress_remaining: float = 1.0):
        """
        æ‰§è¡Œä¸€æ­¥è®­ç»ƒæ›´æ–°
        
        å‚æ•°:
            batch: è®­ç»ƒæ•°æ®æ‰¹æ¬¡å­—å…¸
            mini_batch: mini-batchå¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ•´ä¸ªbatch
        
        å¼ é‡ç»´åº¦è¯´æ˜:
        - obs: tuple of (node_x, pad_mask, cur_idx, vps, action_mask, adj)
        - act_idx: [batch_size] - åŠ¨ä½œç´¢å¼•
        - rewards: [batch_size, 1] - å¥–åŠ±
        - next_obs: tuple æ ¼å¼åŒobs
        - dones: [batch_size, 1] - ç»“æŸæ ‡å¿—
        """
        self.step += 1
        log = {}
        diff_loss_dict = {}

        # æå–æ‰¹æ¬¡æ•°æ®
        obs = [
            batch['node_inputs'],
            batch['node_padding_mask'],
            batch['current_index'],
            batch['viewpoints'],
            batch['viewpoint_padding_mask'],
            batch['adj_list']
        ]
        
        next_obs = [
            batch['next_node_inputs'],
            batch['next_node_padding_mask'],
            batch['next_current_index'],
            batch['next_viewpoints'],
            batch['next_viewpoint_padding_mask'],
            batch['next_adj_list']
        ]
        
        act_idx = batch['actions']
        reward = batch['rewards'].unsqueeze(1)
        not_done = (~batch['dones']).float().unsqueeze(1)
        next_act_dataset = batch.get('next_actions', None)  # æ•°æ®é›†ä¸­çš„çœŸå®ä¸‹ä¸€æ­¥åŠ¨ä½œ

        batch_size = act_idx.shape[0]

        # ç¡®å®šæ˜¯å¦ä½¿ç”¨mini-batchä»¥åŠå¦‚ä½•åˆ’åˆ†
        if mini_batch is None or mini_batch >= batch_size:
            slices = [(0, batch_size)]
        else:
            slices = [(i, min(i + mini_batch, batch_size)) for i in range(0, batch_size, mini_batch)]
            
        # æ¸…ç©ºæ‰€æœ‰æ¢¯åº¦
        self.critic_opt.zero_grad(set_to_none=True)
        if self.step % self.actor_update_freq == 0:
            self.actor_opt.zero_grad(set_to_none=True)

        # ä¸€æ¬¡æ€§è®¡ç®—è‡ªé€‚åº”æƒé‡
        adaptive_info = self.get_adaptive_weights(progress_remaining)

        # ç´¯ç§¯æŸå¤±
        total_critic_loss = 0.0
        total_actor_loss = 0.0
        # ç´¯ç§¯å„ä¸ªactoræŸå¤±åˆ†é‡
        total_bc_loss = 0.0
        total_ql_loss = 0.0
        total_entropy = 0.0
        
        # å¯¹æ¯ä¸ªmini-batchè¿›è¡Œå¤„ç†
        for beg, end in slices:
            mb_size = end - beg
            
            # æå–mini-batchæ•°æ®
            mb_obs = [o[beg:end] for o in obs]
            mb_next_obs = [o[beg:end] for o in next_obs]
            mb_act_idx = act_idx[beg:end]
            mb_reward = reward[beg:end]
            mb_not_done = not_done[beg:end]

            # å…³é”®ä¿®æ”¹ï¼šä»batchä¸­ç›´æ¥æå–ä¸‹ä¸€æ­¥åŠ¨ä½œ
            mb_next_act_dataset = next_act_dataset[beg:end] if next_act_dataset is not None else None

            # ========== Critic update ========= #
            q1, q2 = self.critic(mb_obs)          # each [B,K]ï¼Œç°åœ¨æ˜¯äºŒç»´
            q1a = q1.gather(1, mb_act_idx.unsqueeze(1))  # [B,1]ï¼Œä¸éœ€è¦å†squeeze
            q2a = q2.gather(1, mb_act_idx.unsqueeze(1))

            with torch.no_grad():
                # å…³é”®ä¿®å¤ï¼šé€‰æ‹©ä½¿ç”¨æ•°æ®é›†åŠ¨ä½œè¿˜æ˜¯actoré‡‡æ ·åŠ¨ä½œ
                if self.use_dataset_actions and mb_next_act_dataset is not None:
                    # [A] ä½¿ç”¨æ•°æ®é›†ä¸­çš„çœŸå®ä¸‹ä¸€æ­¥åŠ¨ä½œï¼ˆå¼ºçƒˆæ¨èçš„ç¦»çº¿RLåšæ³•ï¼‰
                    next_act = mb_next_act_dataset
                    log["using_dataset_actions"] = 1.0
                else:
                    # ä¼ ç»Ÿæ–¹å¼ï¼šä½¿ç”¨actoré‡‡æ ·ï¼ˆå¯èƒ½å¯¼è‡´OODé—®é¢˜ï¼‰
                    next_act = self.actor_module.sample(obs=mb_next_obs, padding_mask=mb_next_obs[4])
                    log["using_dataset_actions"] = 0.0
                
                # è®¡ç®—ç›®æ ‡Qå€¼
                q1_t, q2_t = self.critic_tgt(mb_next_obs)
                q1_next = q1_t.gather(1, next_act.unsqueeze(1))   # ä¸éœ€è¦å†squeeze
                q2_next = q2_t.gather(1, next_act.unsqueeze(1))
                target_q = mb_reward + mb_not_done * self.discount * torch.min(q1_next, q2_next)

            # Critic æŸå¤±
            critic_loss = self.compute_critic_loss(q1a, q2a, target_q)
            critic_loss.backward()
            total_critic_loss += critic_loss.item() * mb_size / batch_size

            # ==================== Actorè®­ç»ƒ ==================== #
            if self.step % self.actor_update_freq == 0:
                bc_loss, diff_loss_dict = self.actor_module.loss(mb_act_idx, mb_obs[4], mb_obs)
                
                # å¯å¾®åˆ†é‡‡æ ·è·å–åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
                action_probs = self.actor_module.sample_differentiable(
                    obs=mb_obs, 
                    padding_mask=mb_obs[4], 
                    temperature=self.current_temperature,
                )  # [B, K] - å®Œå…¨å¯å¾®åˆ†çš„æ¦‚ç‡åˆ†å¸ƒ
                
                # é‡è¦ä¿®å¤ï¼šå®Œå…¨åˆ†ç¦»criticå‚æ•°çš„æ¢¯åº¦
                with torch.no_grad():
                    q1_det, q2_det = self.critic_module(mb_obs)  # each [B,K]
                    q_min = torch.min(q1_det, q2_det)  # [B,K] - å®Œå…¨æ— æ¢¯åº¦
                    # è·å–baseline Qå€¼ç”¨äºå½’ä¸€åŒ–
                    q_other = self.critic_module.q_min(mb_obs, mb_act_idx)  # [B,1] - baseline

                # ä½¿ç”¨ç¨³å®šçš„QLæŸå¤±è®¡ç®—ï¼ˆå¸¦ç†µæ­£åˆ™åŒ–ï¼‰
                ql_loss, entropy = self.compute_ql_loss_stable(action_probs, q_min, q_other)
                
                # ç»Ÿä¸€è¯­ä¹‰ï¼šactor_bc_coefæ§åˆ¶BCæŸå¤±æƒé‡ï¼ŒQLæŸå¤±æƒé‡å›ºå®šä¸º1
                actor_loss = ql_loss + self.actor_bc_coef * bc_loss

                actor_loss.backward()
                total_actor_loss += actor_loss.item() * mb_size / batch_size
                # ç´¯ç§¯å„ä¸ªæŸå¤±åˆ†é‡
                total_bc_loss += bc_loss.item() * mb_size / batch_size
                total_ql_loss += ql_loss.item() * mb_size / batch_size
                total_entropy += entropy.item() * mb_size / batch_size

        # æ‰€æœ‰mini-batchå¤„ç†å®Œåï¼Œè¿›è¡Œæ¢¯åº¦è£å‰ªå’Œå‚æ•°æ›´æ–°
        if self.grad_clip > 0:
            nn.utils.clip_grad_norm_(self.critic.parameters(), self.grad_clip)
            if self.step % self.actor_update_freq == 0:
                nn.utils.clip_grad_norm_(self.actor.parameters(), self.grad_clip)
                
        # æ›´æ–°å‚æ•°
        self.critic_opt.step()
        if self.step % self.actor_update_freq == 0:
            self.actor_opt.step()
            log["actor_loss"] = total_actor_loss
            log["bc_loss"] = total_bc_loss
            log["q_loss"] = total_ql_loss
            log["entropy"] = total_entropy  # æ–°å¢ï¼šè®°å½•ç†µå€¼
            log["entropy_reg_weight"] = self.entropy_reg_weight  # æ–°å¢ï¼šè®°å½•ç†µæ­£åˆ™åŒ–æƒé‡

        log.update(diff_loss_dict)
        
        # æ·»åŠ ç¨³å®šåŒ–ç›¸å…³æ—¥å¿—
        log["use_smooth_loss"] = float(self.use_smooth_loss)
        log["use_dataset_actions"] = float(self.use_dataset_actions)
        
        # æ·»åŠ æ¸©åº¦å’Œé‡‡æ ·æ–¹å¼åˆ°æ—¥å¿—
        log["temperature"] = self.current_temperature
        
        # ---------- è‡ªé€‚åº”æƒé‡æ›´æ–° ---------- #
        log.update(adaptive_info)  # å°†æ‰€æœ‰è‡ªé€‚åº”æ›´æ–°ä¿¡æ¯æ·»åŠ åˆ°æ—¥å¿—ä¸­
        
        # ---------- å¯¹é½trainer_ddp.pyæœŸæœ›çš„æ—¥å¿—é”®å€¼ ---------- #
        # æ·»åŠ trainer_ddp.pyæœŸæœ›çš„é”®å€¼ï¼Œç¡®ä¿æ—¥å¿—å®Œæ•´æ€§
        log["critic_loss"] = total_critic_loss  # æ˜ å°„value_lossåˆ°critic_loss
        log["diff/loss_ce"] = diff_loss_dict.get('diff/loss_ce', 0.0)  # æ‰©æ•£å™ªå£°æŸå¤±
        log["entropy_loss"] = -total_entropy if total_entropy > 0 else 0.0  # ç†µæŸå¤±ï¼ˆè´Ÿç†µï¼‰
        log["diff/loss_consistency"] = diff_loss_dict.get('diff/loss_consistency', 0.0)  # ä¸€è‡´æ€§æŸå¤±ä½œä¸ºKLæŸå¤±
        log["step"] = self.step  # å½“å‰è®­ç»ƒæ­¥æ•°
        
        # åˆ›å»ºadaptive_weightså­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰è‡ªé€‚åº”æƒé‡ä¿¡æ¯
        adaptive_weights = {
            "actor_bc_coef": adaptive_info.get("actor_bc_coef", self.actor_bc_coef),
            "entropy_reg_weight": self.entropy_reg_weight,
            "temperature": self.current_temperature,
            "actor_lr": adaptive_info.get("actor_lr", 0.0),
            "critic_lr": adaptive_info.get("critic_lr", 0.0),
            "lr_factor": adaptive_info.get("lr_factor", 1.0),
        }
        log["adaptive_weights"] = adaptive_weights
        
        # ---------- EMA & target-net ---------- #
        if self.step >= self.step_start_ema and self.step % self.update_ema_every == 0:
            self.ema.update(self.actor, self.actor)  # keeps same weights for clarity
        
        with torch.no_grad():
            for p, p_t in zip(self.critic.parameters(), self.critic_tgt.parameters()):
                p_t.mul_(1 - self.tau).add_(p.data, alpha=self.tau)

        return log

    def _get_lr(self, optimizer):
        """è·å–ä¼˜åŒ–å™¨çš„å½“å‰å­¦ä¹ ç‡"""
        for param_group in optimizer.param_groups:
            return param_group['lr']
        return None

    # ---------------- Load / State Dict --------------- #

    def load_state_dict(self, state_dict: Dict[str, Dict[str, torch.Tensor]]):
        """
        åŠ è½½æ¨¡å‹çš„çŠ¶æ€å­—å…¸
        å‚æ•°:
            state_dict: åŒ…å«æ¨¡å‹å‚æ•°çš„çŠ¶æ€å­—å…¸
        """
        if isinstance(self.actor, nn.parallel.DistributedDataParallel):
            self.actor.module.load_state_dict(state_dict["actor"])
        else:
            self.actor.load_state_dict(state_dict["actor"])
        if isinstance(self.critic, nn.parallel.DistributedDataParallel):
            self.critic.module.load_state_dict(state_dict["critic"])
        else:
            self.critic.load_state_dict(state_dict["critic"])
        if isinstance(self.critic_tgt, nn.parallel.DistributedDataParallel):
            self.critic_tgt.module.load_state_dict(state_dict["critic_tgt"])
        else:
            self.critic_tgt.load_state_dict(state_dict["critic_tgt"])
        # æ›´æ–°æ€»æ­¥æ•°
        self.step = state_dict["step"]

    def get_state_dict(self, device: str = "cpu"):
        """
        è·å–å½“å‰æ¨¡å‹çš„çŠ¶æ€å­—å…¸ï¼Œç”¨äºä¿å­˜å’ŒDDPè®­ç»ƒ
        
        å‚æ•°:
            device: ç›®æ ‡è®¾å¤‡ï¼ˆé»˜è®¤ä¸º"cpu"ï¼‰
        è¿”å›:
            åŒ…å«æ¨¡å‹å‚æ•°çš„çŠ¶æ€å­—å…¸
        """
        # å¤„ç†å¯èƒ½çš„DDPåŒ…è£…
        actor_state = self.actor.module.state_dict() if isinstance(self.actor, nn.parallel.DistributedDataParallel) else self.actor.state_dict()
        critic_state = self.critic.module.state_dict() if isinstance(self.critic, nn.parallel.DistributedDataParallel) else self.critic.state_dict()
        critic_tgt_state = self.critic_tgt.module.state_dict() if isinstance(self.critic_tgt, nn.parallel.DistributedDataParallel) else self.critic_tgt.state_dict()
        actor_opt = self.actor_opt.state_dict()
        critic_opt = self.critic_opt.state_dict()
        model_state_dict = {
            'actor': actor_state,
            'critic': critic_state,
            'critic_tgt': critic_tgt_state,
            'actor_opt': actor_opt,
            'critic_opt': critic_opt,
            'step': self.step
        }
        
        # å°†çŠ¶æ€å­—å…¸ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        for key in model_state_dict:
            if key != 'step':  # è·³è¿‡éå¼ é‡å€¼
                for param_key in model_state_dict[key]:
                    model_state_dict[key][param_key] = model_state_dict[key][param_key].to(device)
        
        return model_state_dict

    def get_adaptive_weights(self, progress_remaining):
        """
        è‡ªé€‚åº”æƒé‡æ›´æ–°ï¼šç»Ÿä¸€æ›´æ–°å­¦ä¹ ç‡ã€é‡‡æ ·æ¸©åº¦å’ŒBCç³»æ•°
        
        å‚æ•°:
            progress_remaining: å‰©ä½™è®­ç»ƒè¿›åº¦æ¯”ä¾‹ (0.0 åˆ° 1.0)
        
        è¿”å›:
            åŒ…å«æ‰€æœ‰æ›´æ–°ä¿¡æ¯çš„å­—å…¸
        """
        # ç¡®ä¿è¿›åº¦å€¼åˆæ³•
        progress_remaining = np.clip(progress_remaining, 0.00001, 1.0)
        progress = 1.0 - progress_remaining  # è®­ç»ƒè¿›åº¦ 0->1
        
        update_info = {}
        
        # ==================== é‡‡æ ·æ¸©åº¦æ›´æ–° (çº¿æ€§é€€ç«) ====================
        # çº¿æ€§é€€ç«ï¼šä»åˆå§‹æ¸©åº¦é€€ç«åˆ°æœ€ç»ˆæ¸©åº¦
        decay_progress = min(progress * self.temperature_decay_steps / max(self.step, 1), 1.0)
        self.current_temperature = self.temperature_start + (self.temperature_end - self.temperature_start) * decay_progress
        
        # æ·»åŠ é‡‡æ ·æ¸©åº¦ä¿¡æ¯åˆ°è¿”å›å­—å…¸
        update_info.update({
            "temperature": self.current_temperature,
            "temperature_progress": decay_progress
        })
        
        # ==================== å­¦ä¹ ç‡æ›´æ–° (ç»å°”å…¹æ›¼é€€ç«) ====================
        # ç»å°”å…¹æ›¼é€€ç«å‚æ•°
        initial_temperature = 1.0  # åˆå§‹æ¸©åº¦
        min_lr_factor = 0.01      # æœ€å°å­¦ä¹ ç‡å› å­
        
        # è®¡ç®—å½“å‰æ¸©åº¦: T(t) = T0 / log(e + t)
        normalized_progress = progress * 20
        current_temperature = initial_temperature / np.log(np.e + normalized_progress) 
        
        # è®¡ç®—å­¦ä¹ ç‡å› å­ (èŒƒå›´åœ¨min_lr_factoråˆ°1.0ä¹‹é—´)
        lr_factor = min_lr_factor + (1.0 - min_lr_factor) * (current_temperature / initial_temperature)
        
        # æ›´æ–°actorå­¦ä¹ ç‡
        for param_group in self.actor_opt.param_groups:
            param_group['lr'] = self.initial_actor_lr * lr_factor
            
        # æ›´æ–°criticå­¦ä¹ ç‡
        for param_group in self.critic_opt.param_groups:
            param_group['lr'] = self.initial_critic_lr * lr_factor
        
        # ==================== BCç³»æ•°æ›´æ–° (æŒ‡æ•°è¡°å‡) ====================
        # DDQLæ¨¡å¼ï¼šä½¿ç”¨æŒ‡æ•°è¡°å‡ï¼Œåœ¨è®­ç»ƒåæœŸå‡å°‘BCæŸå¤±æƒé‡ï¼Œè®©QLæŸå¤±èµ·ä¸»å¯¼ä½œç”¨
        decay_rate = 3.0  # è¡°å‡é€Ÿç‡ï¼Œå€¼è¶Šå¤§è¡°å‡è¶Šå¿«
        min_coef_factor = 0.01  # æœ€å°ç³»æ•°å› å­ï¼Œé˜²æ­¢BCæŸå¤±å®Œå…¨æ¶ˆå¤±
        
        # æŒ‡æ•°è¡°å‡å…¬å¼: coef = initial_coef * (min_factor + (1 - min_factor) * exp(-decay_rate * progress))
        decay_factor = min_coef_factor + (1 - min_coef_factor) * np.exp(-decay_rate * progress)
        current_coef = self.initial_actor_bc_coef * decay_factor

        # æ›´æ–°å½“å‰çš„actor_bc_coef
        self.actor_bc_coef = current_coef

        # æ·»åŠ å­¦ä¹ ç‡ä¿¡æ¯åˆ°è¿”å›å­—å…¸
        update_info.update({
            "actor_lr": self.initial_actor_lr * lr_factor,
            "critic_lr": self.initial_critic_lr * lr_factor,
            "lr_factor": lr_factor,
            "lr_temperature": current_temperature
        })
        
        # æ·»åŠ BCç³»æ•°ä¿¡æ¯åˆ°è¿”å›å­—å…¸
        update_info.update({
            "actor_bc_coef": current_coef,
            "actor_bc_coef_decay_factor": decay_factor,
            "actor_bc_coef_progress": progress
        })
        
        return update_info

# ======================= æ¨¡å‹åˆå§‹åŒ–å‡½æ•° ======================= #
def initialize_ddql(config, node_dim, rank, world_size):
    """
    åˆå§‹åŒ–DDPæ¨¡å¼ä¸‹çš„DDQLæ¨¡å‹
    """
    device = torch.device(config.device) if hasattr(config, 'device') else (
        torch.device(f"cuda:{rank}") if world_size > 1 else torch.device("cpu"))
    
    # æ ¹æ®é…ç½®é€‰æ‹©denoiserç±»å‹
    denoiser = create_pointer_denoiser(
        node_dim=node_dim,
        embed_dim=config.gnn_hidden_dim,
        max_actions=config.viewpoint_padding_size,
        T=config.n_timesteps
    ).to(device)

    # åˆ›å»ºActor(Diffusion)ï¼Œä¼ å…¥æ¨¡å¼é…ç½®å’Œä½™å¼¦è°ƒåº¦
    actor = DiscreteDiffusion(
        num_actions=config.viewpoint_padding_size,
        model=denoiser,
        T=config.n_timesteps,
        schedule='cosine',
    ).to(device)
    
    # ä¸ºactoræ·»åŠ é‡‡æ ·é…ç½®å±æ€§
    actor.temperature_start = getattr(config, 'temperature_start', 1.0)
    actor.temperature_end = getattr(config, 'temperature_end', 0.1)
    actor.temperature_decay_steps = getattr(config, 'temperature_decay_steps', 50000)
    
    # åˆ›å»ºCritic
    critic = TwinQ(node_dim, config.gnn_hidden_dim).to(device)
    
    # åŒ…è£…æˆDDPæ¨¡å‹ï¼ˆç¡®ä¿T=0æ—¶ä¹Ÿèƒ½æ­£ç¡®åŒ…è£…ï¼‰
    if world_size > 1:
        if rank == 0:
            print(f"ğŸ”— åŒ…è£…ä¸ºDDPæ¨¡å‹ï¼Œrank={rank}, world_size={world_size}")
        actor = DDP(actor, device_ids=[rank], output_device=rank, find_unused_parameters=True)
        critic = DDP(critic, device_ids=[rank], output_device=rank, find_unused_parameters=True)
        
    # åˆ›å»ºä¼˜åŒ–å™¨
    actor_opt = Adam(actor.parameters(), lr=config.learning_rate)
    critic_opt = Adam(critic.parameters(), lr=config.learning_rate)
        
    # åˆ›å»ºDiffusionQLæ¨¡å‹
    model = DiffusionGraphQL(
        actor=actor,
        actor_opt=actor_opt,
        critic=critic,
        critic_opt=critic_opt,
        device=device,
        discount=getattr(config, 'discount', 0.99),
        tau=getattr(config, 'tau', 0.005),
        actor_bc_coef=getattr(config, 'actor_bc_coef', 1.0),
        ema_decay=getattr(config, 'ema_decay', 0.995),
        step_start_ema=getattr(config, 'step_start_ema', 1000),
        update_ema_every=getattr(config, 'update_ema_every', 5),
        grad_clip=getattr(config, 'clip_grad_norm', 1.0),
        rank=rank,
        world_size=world_size,
        actor_update_freq=getattr(config, 'actor_update_freq', 1),
        # æ–°å¢ï¼šcriticç¨³å®šåŒ–é…ç½®
        use_dataset_actions=getattr(config, 'use_dataset_actions', True),
        use_smooth_loss=getattr(config, 'use_smooth_loss', True),
        huber_delta=getattr(config, 'huber_delta', 1.0),
        # æ–°å¢ï¼šç†µæ­£åˆ™åŒ–é…ç½®
        entropy_reg_weight=getattr(config, 'entropy_reg_weight', 0.01),
    )
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if config.load_model_path:
        if rank == 0:
            print(f"ğŸ“‚ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {config.load_model_path}")
        model_state_dict = torch.load(config.load_model_path, map_location=device)
        model.load_state_dict(model_state_dict)
    if rank == 0:
        print(f"ğŸ¯ å¤šæ­¥å»å™ªæ¨¡å‹åˆå§‹åŒ–å®Œæˆ:")
    return model